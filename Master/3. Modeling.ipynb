{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## import base classifiers\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## to measure run-times\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "## the usual tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "## to save and load models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the datasets\n",
    "year_list = [2008,2012,2016,2020]\n",
    "df_list = [pd.read_csv(f'../Master/clean_data/data{year}.csv') for year in year_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here all the features we use:\n",
    "all_features = ['labor%', 'armed%', 'empl%', 'unempl%', 'poverty%', 'income_median', 'income_percapita',\n",
    "'income_10%', 'income_10-15%', 'income_15-25%', 'income_25%', 'single%',\n",
    "'married%', 'marital_ratio', 'sepdiv%', 'widow%', 'edu_low%',\n",
    "'edu_mid%', 'edu_high%', 'edulow_age18%', 'edulow_age45%',\n",
    "'edulow_age65%', 'edumid_age18%', 'edumid_age45%', 'edumid_age65%',\n",
    "'eduhigh_age18%', 'eduhigh_age45%', 'eduhigh_age65%', 'age_18%',\n",
    "'age_45%', 'age_65%', 'wht%', 'wht_m%', 'wht_f%', 'blk%', 'blk_m%',\n",
    "'blk_f%', 'other%', 'other_m%', 'other_f%', 'native%', 'hispanic%',\n",
    "'pop_density', 'houses_density', 'pop_tot', 'area_sqmiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of X and y (by year); we will partition these in various\n",
    "#ways to carry out model-selection (via cross validation)\n",
    "X_list = [df[all_features] for df in df_list]\n",
    "y_list = [df['vote_diff%'] for df in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 5 Kfold objects with different random states (all nice primes!)\n",
    "kfold_list = [KFold(n_splits = 10,\n",
    "                    shuffle = True,\n",
    "                    random_state = r_state) for r_state in [13,17,43,61,101]]\n",
    "\n",
    "#Create a list with four entries (one per election year)\n",
    "#Each entry consists of the 5*10=50 CV splits coming from the 5 kfold objects\n",
    "cv_list = []\n",
    "for i in range(4):\n",
    "    ith_cv = []\n",
    "    X = X_list[i]\n",
    "    y = y_list[i]\n",
    "    for n,kfold in enumerate(kfold_list):\n",
    "        #Generate list of 10 (X_train,X_test) pairs using the n-th kffold object\n",
    "        for train_index,test_index in kfold.split(df_list[i]):\n",
    "            X_tr = X.iloc[train_index]\n",
    "            X_te = X.iloc[test_index]\n",
    "            y_tr = y.iloc[train_index]\n",
    "            y_te = y.iloc[test_index]\n",
    "            ith_cv.append((X_tr,y_tr,X_te,y_te))\n",
    "    cv_list.append(ith_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of subsets of [0,1,2,3]\n",
    "#For each subset, we will train our models on the union of \n",
    "#all the data from the corresponding years\n",
    "subset3_list = [[0,1,2],[0,1,3],[0,2,3],[1,2,3]]\n",
    "subset2_list = [[0,1],[0,2],[0,3],\n",
    "             [2,3],[1,3],[1,2]]\n",
    "subset1_list = [[3],[2],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next method inputs a subset of [0,1,2,3], and it concatenates\n",
    "#the CV splits correpsonding to those datasets.\n",
    "#Thus, the output consists of 50 CV splits of the union of these datasets.\n",
    "#This is done to ensure that each CV split draws equally from all \n",
    "#the years corresponding to that subset\n",
    "def concat_cv(subset):\n",
    "    concat_cv_list = []\n",
    "    scaler = StandardScaler()\n",
    "    for n in range(50):     #range over the 50 (train,test) \n",
    "        #Concatenate the nth train,test pair of the i-th year, for all i in subset\n",
    "        X_tr = pd.concat([cv_list[i][n][0] for i in subset])\n",
    "        y_tr = pd.concat([cv_list[i][n][1] for i in subset])\n",
    "        X_te = pd.concat([cv_list[i][n][2] for i in subset])\n",
    "        y_te = pd.concat([cv_list[i][n][3] for i in subset])\n",
    "        #Scale the train,test pairs\n",
    "        scaler.fit_transform(X_tr)\n",
    "        scaler.transform(X_te)\n",
    "        concat_cv_list.append((X_tr,y_tr,X_te,y_te))\n",
    "    return concat_cv_list\n",
    "\n",
    "#A simple and useful method to obtain the desired (X,y) depending\n",
    "#on the subset of years we are training on.\n",
    "def concat_main(subset):\n",
    "    if subset in subset3_list + subset2_list:\n",
    "        return(pd.concat([X_list[i] for i in subset]),\n",
    "               pd.concat([y_list[i] for i in subset]))\n",
    "    else:\n",
    "        i = subset[0]\n",
    "        return X_list[i],y_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'vote_diff%' feature (for a particular county) is positive if \n",
    "#republicans win, and it is negative if Democrats win. Thus, the predicted\n",
    "#vote difference fails to predict the winner exactly when it differs from \n",
    "#the true vote difference in sign; we use this fact to compute the accuracy\n",
    "#of our predicted vote difference.\n",
    "def vote_diff_acc(y_pred,y_true):\n",
    "    y = y_pred * y_true\n",
    "    return len(y[y >= 0])/len(y)\n",
    "\n",
    "#The next method is used to tune hyperparameters.\n",
    "#For each of the three models, it fits and computes the CV accuracy\n",
    "#score as the corresponding hyperparameter ranges over a prescribed set of\n",
    "#values. Then, it chooses the model with best mean CV accuracy and\n",
    "#returns the associated hyperparameter value and some accuracy statistics\n",
    "#(in the form of a dictionary)\n",
    "def my_cv_scorer(subset,   \n",
    "                 model,     #'ridge', 'knn', or 'rfr'\n",
    "                 feature_list=all_features):    #optional: choose some subset of features\n",
    "\n",
    "    if model=='ridge':\n",
    "        param = 'alpha'\n",
    "        param_list = [0.05*k for k in range(20)]\n",
    "        model_list = [Ridge(alpha=alpha,\n",
    "                            solver='lsqr') for alpha in param_list]\n",
    "        \n",
    "    elif model=='knn':\n",
    "        param = 'k'\n",
    "        param_list = range(3,16*(4 - len(subset)) - 1,2)\n",
    "        model_list = [KNeighborsRegressor(k) for k in param_list]\n",
    "        \n",
    "    elif model=='rfr':\n",
    "        param = 'max_features'\n",
    "        if len(subset)==1:\n",
    "            param_list = [0.6,0.7,0.8,0.9]\n",
    "        elif len(subset)==2:\n",
    "            param_list = [0.4,0.5,0.6,0.7]\n",
    "        else: \n",
    "            param_list = [0.3,0.4,0.5,0.6]\n",
    "        model_list = [RandomForestRegressor(n_estimators=100*len(subset),\n",
    "                                            criterion='squared_error',\n",
    "                                            max_depth=15,\n",
    "                                            max_features=n,\n",
    "                                            n_jobs=-1,\n",
    "                                            bootstrap=True,\n",
    "                                            random_state=42) for n in param_list]\n",
    "    \n",
    "    mse_list = []\n",
    "    avgacc_list = []\n",
    "    lowestacc_list = []\n",
    "    highestacc_list = []\n",
    "\n",
    "    #get list of cv splits\n",
    "    concat_cv_list = [(X_tr[feature_list],y_tr,X_te[feature_list],y_te) for X_tr,y_tr,X_te,y_te in concat_cv(subset)]\n",
    "\n",
    "    for modl in model_list:\n",
    "        scores_cv = []\n",
    "        mses_cv = []\n",
    "        \n",
    "        for X_train,y_train, X_test,y_test in concat_cv_list:\n",
    "            modl.fit(X_train,y_train)                           #fit on the train set\n",
    "            my_pred = modl.predict(X_test)                      #predict on the test set\n",
    "            scores_cv.append(vote_diff_acc(y_test,my_pred))     #Compute accuracy score, add to list of scores     \n",
    "            mses_cv.append(mse(y_test,my_pred))                 #Compute mse, add to list of mses for fixed k\n",
    "\n",
    "        avgacc_list.append(round(np.mean(scores_cv),5))          #Add average CV accuracy\n",
    "        mse_list.append(round(np.mean(mses_cv),5))              #Add average CV mse\n",
    "        lowestacc_list.append(round(np.min(scores_cv),5))       #Add lowest CV accuracy\n",
    "        highestacc_list.append(round(np.max(scores_cv),5))      #Add highest CV accuracy\n",
    "\n",
    "    index = np.argmax(avgacc_list)\n",
    "    best_model = model_list[index]\n",
    "\n",
    "    if model =='ridge':\n",
    "        fimp_df = pd.DataFrame({'feature':feature_list,\n",
    "                                'importance': best_model.coef_,\n",
    "                                'imp_abs': np.abs(best_model.coef_)}).sort_values(by='imp_abs',ascending=False,ignore_index=True)\n",
    "        fimp_df.drop(inplace=True,columns=['imp_abs'])\n",
    "    elif model=='rfr':\n",
    "        fimp_df = pd.DataFrame({'feature': feature_list,\n",
    "                                'importance': best_model.feature_importances_}).sort_values(by='importance',ascending=False,ignore_index=True)\n",
    "    else:\n",
    "        fimp_df = pd.DataFrame({'feature': feature_list,\n",
    "                                'importance': 0})\n",
    "\n",
    "    cv_results = {param: param_list[index],\n",
    "                  'mse': mse_list[index],\n",
    "                  'Average accuracy': avgacc_list[index],\n",
    "                  'Highest accuracy': highestacc_list[index],\n",
    "                  'Lowest accuracy': lowestacc_list[index],\n",
    "                  'Features used': fimp_df['feature'],\n",
    "                  'Importance': fimp_df['importance']}\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next method carries out feature selection for ridge linear regression\n",
    "#The algorithm is recursive feature elimination with cross-validation\n",
    "#(using the my_cv_scorer method)\n",
    "def ridge_selector(subset):\n",
    "    best_alpha_list = []\n",
    "    avgacc_list=[]\n",
    "    highestacc_list = []\n",
    "    lowestacc_list = []\n",
    "    feats_used = []\n",
    "    feats_imp = []\n",
    "\n",
    "    feats = all_features\n",
    "    while(len(feats) >= 10):\n",
    "        cv_results = my_cv_scorer(subset,'ridge', feats)\n",
    "        best_alpha_list.append(cv_results['alpha'])\n",
    "        avgacc_list.append(cv_results['Average accuracy'])\n",
    "        highestacc_list.append(cv_results['Highest accuracy'])\n",
    "        lowestacc_list.append(cv_results['Lowest accuracy'])\n",
    "        feats_used.append(cv_results['Features used'])\n",
    "        feats_imp.append(cv_results['Importance'])\n",
    "        feats = cv_results['Features used'].to_list()[:-1]\n",
    "    ridge_results_df =  pd.DataFrame({'alpha': best_alpha_list,\n",
    "                         'Average accuracy': avgacc_list,\n",
    "                         'Highest accuracy': highestacc_list,\n",
    "                         'Lowest accuracy': lowestacc_list,\n",
    "                         'Features used': feats_used,\n",
    "                         'Importance': feats_imp})\n",
    "    ridge_results_df.sort_values(by='Lowest accuracy',ascending=False,inplace=True)\n",
    "    ridge_results_df.sort_values(by='Average accuracy',ascending=False,inplace=True)\n",
    "\n",
    "    return ridge_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above for the kNN model, but since the latter does not have a \n",
    "#feature importance function (or something analogous to that), to\n",
    "#determine the order in which we eliminate features, we simply\n",
    "#use the feature importance obtained from the top scoring ridge model that\n",
    "#uses all the features.\n",
    "def knn_selector(subset):\n",
    "    best_k_list = []\n",
    "    avgacc_list=[]\n",
    "    highestacc_list = []\n",
    "    lowestacc_list = []\n",
    "    feats_used = []\n",
    "    \n",
    "    feats = my_cv_scorer(subset,'ridge')['Features used'].to_list()\n",
    "\n",
    "    while(len(feats) >= 15):\n",
    "        cv_results = my_cv_scorer(subset,'knn',feature_list = feats)\n",
    "        # best_ridges.append(results_df.loc[0,'Model'])\n",
    "        best_k_list.append(cv_results['k'])\n",
    "        avgacc_list.append(cv_results['Average accuracy'])\n",
    "        highestacc_list.append(cv_results['Highest accuracy'])\n",
    "        lowestacc_list.append(cv_results['Lowest accuracy'])\n",
    "        feats_used.append(cv_results['Features used'])\n",
    "        del feats[-1]\n",
    "\n",
    "    knn_results_df = pd.DataFrame({'k': best_k_list,\n",
    "                                   'Average accuracy': avgacc_list,\n",
    "                                   'Highest accuracy': highestacc_list,\n",
    "                                   'Lowest accuracy': lowestacc_list,\n",
    "                                   'Features used': feats_used})\n",
    "    \n",
    "    knn_results_df.sort_values(by='Lowest accuracy',ascending=False,inplace=True)\n",
    "    knn_results_df.sort_values(by='Average accuracy',ascending=False,inplace=True)\n",
    "    \n",
    "    return knn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do not do any feature selection for the random forest regressor\n",
    "#This method is included only for naming consistency.\n",
    "def rfr_selector(subset):\n",
    "    return my_cv_scorer(subset,'rfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next method inputs a subset of [0,1,2,3]\n",
    "#It uses the selector methods to get the \"best\" models (within the scope\n",
    "#of the hyperparameters are features we investigate). It fits these models\n",
    "#to the union of the datasets corresponding to \"subset\", and it saves\n",
    "#csv's containing the feature importances and final predictions + results. \n",
    "def combined_regressor(subset):\n",
    "\n",
    "    #Create the main train/test sets\n",
    "    X_train,y_train = concat_main(subset)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "\n",
    "    #Used in the filenames of things we save\n",
    "    name = ''.join(f'_{year_list[i]%100}' for i in subset)\n",
    "\n",
    "    ####################################################################################\n",
    "    #Ridge\n",
    "    start_time = time.monotonic()\n",
    "    best_ridge = ridge_selector(subset)\n",
    "    end_time = time.monotonic()\n",
    "    ridge_time = timedelta(seconds=end_time - start_time)\n",
    "    alpha = best_ridge.loc[0,'alpha']\n",
    "    ridge_feats = best_ridge.loc[0,'Features used']\n",
    "    ridge_cv_acc = best_ridge.loc[0,'Average accuracy']\n",
    "    print(f'ridge regressor computed in {ridge_time} seconds.')\n",
    "    print(f'Best CV accuracy {ridge_cv_acc} achieved with alpha = {alpha}.')\n",
    "\n",
    "    ridge = Ridge(alpha)\n",
    "    ridge.fit(X_train[ridge_feats],y_train)\n",
    "    pickle.dump(ridge,open(f'models/ridge{name}.sav', 'wb'))\n",
    "\n",
    "    fimp_ridge = pd.DataFrame({'feature':ridge_feats,\n",
    "                            'importance': ridge.coef_,\n",
    "                            'imp_abs': np.abs(ridge.coef_)}).sort_values(by='imp_abs',ascending=False,ignore_index=True)\n",
    "    fimp_ridge.drop(inplace=True,columns=['imp_abs'])\n",
    "    fimp_ridge.to_csv(f'results/fimp_ridge{name}.csv', index=False)\n",
    "\n",
    "    ####################################################################################\n",
    "    #knn\n",
    "    start_time = time.monotonic()\n",
    "    best_knn = knn_selector(subset)\n",
    "    end_time = time.monotonic()\n",
    "    knn_time = timedelta(seconds=end_time - start_time)\n",
    "    k = best_knn.loc[0,'k']\n",
    "    knn_feats = best_knn.loc[0,'Features used']\n",
    "    knn_cv_acc = best_knn.loc[0,'Average accuracy']\n",
    "    print(f'knn regressor computed in {knn_time} seconds.')\n",
    "    print(f'best CV accuracy {knn_cv_acc} achieved with k = {k}.')   \n",
    "    \n",
    "    knn = KNeighborsRegressor(k)\n",
    "    knn.fit(X_train[knn_feats],y_train)\n",
    "    pickle.dump(knn,open(f'models/knn{name}.sav','wb'))\n",
    "\n",
    "    ####################################################################################\n",
    "    #random forest\n",
    "    start_time = time.monotonic()\n",
    "    best_rfr = rfr_selector(subset)\n",
    "    end_time = time.monotonic()\n",
    "    rfr_time = timedelta(seconds=end_time - start_time)\n",
    "    max_feats = best_rfr['max_features']\n",
    "    rfr_cv_acc = best_rfr['Average accuracy']\n",
    "    print(f'random forest regressor computed in {rfr_time} seconds.')\n",
    "    print(f'best CV accuracy {rfr_cv_acc} achieved with max_features = {max_feats}.')\n",
    "    \n",
    "    rfr = RandomForestRegressor(n_estimators=100*len(subset),\n",
    "                                criterion='squared_error',\n",
    "                                max_depth=15,\n",
    "                                max_features=max_feats,\n",
    "                                n_jobs=-1,\n",
    "                                bootstrap=True,\n",
    "                                random_state=42)\n",
    "    rfr.fit(X_train[all_features],y_train)\n",
    "    pickle.dump(rfr,open(f'models/rfr{name}.sav','wb'))\n",
    "\n",
    "    fimp_rfr = pd.DataFrame({'feature': best_rfr['Features used'],\n",
    "                        'importance': rfr.feature_importances_}).sort_values(by='importance',ascending=False,ignore_index=True)\n",
    "    fimp_rfr.to_csv(f'results/fimp_rfr{name}.csv', index=False)\n",
    "\n",
    "    ####################################################################################\n",
    "    #Assembling the final results\n",
    "    results_list = []\n",
    "    comp = list({0,1,2,3} - set(subset))\n",
    "    for j in comp:\n",
    "        X_test = X_list[j]\n",
    "        y_test = y_list[j]\n",
    "        scaler.transform(X_test)\n",
    "\n",
    "        y_ridge = ridge.predict(X_test[ridge_feats])\n",
    "        y_knn = knn.predict(X_test[knn_feats])\n",
    "        y_rfr = rfr.predict(X_test[all_features])\n",
    "\n",
    "        #We form a weighted average of the predictions, giving higher\n",
    "        #weight to the random forest model becuase it tended to perform\n",
    "        #better in testing.\n",
    "        y_avg = (y_ridge + y_knn + 2*y_rfr)/4\n",
    "\n",
    "        pred_df = pd.DataFrame({'ridge':y_ridge,\n",
    "                                'knn': y_knn,\n",
    "                                'rfr' : y_rfr,\n",
    "                                'avg' : y_avg})\n",
    "        pred_df.to_csv(f'results/{year_list[j]}preds_using{name}.csv',index=False)\n",
    "\n",
    "        acc_ridge = round(vote_diff_acc(y_ridge,y_test),3)\n",
    "        acc_knn = round(vote_diff_acc(y_knn,y_test),3)\n",
    "        acc_rfr = round(vote_diff_acc(y_rfr,y_test),3)\n",
    "        acc_avg = round(vote_diff_acc(y_avg,y_test),3)\n",
    "\n",
    "        mse_ridge = round(mse(y_ridge,y_test),3)\n",
    "        mse_knn = round(mse(y_knn,y_test),3)\n",
    "        mse_rfr = round(mse(y_rfr,y_test),3)\n",
    "        mse_avg = round(mse(y_avg,y_test),3)\n",
    "        \n",
    "        results_list.append(pd.DataFrame({'Year' : year_list[j],\n",
    "                                          'Model': [f'Ridge linear (alpha = {alpha})',f'k-Nearest Neighbors (k = {k})',f'Random forest (depth=15,max_features={max_feats})','Averaged'],\n",
    "                                          'Test accuracy': [acc_ridge,acc_knn,acc_rfr,acc_avg],\n",
    "                                          'CV accuracy': [ridge_cv_acc,knn_cv_acc,rfr_cv_acc,'N/A'],\n",
    "                                          'MSE': [mse_ridge,mse_knn,mse_rfr,mse_avg],\n",
    "                                          'Time taken:': [ridge_time,knn_time,rfr_time, 'N/A']}))\n",
    "    pd.concat(results_list).to_csv(f'results/results{name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regressor computed in 0:00:48.929804 seconds.\n",
      "Best CV accuracy 0.853 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:28.110337 seconds.\n",
      "best CV accuracy 0.86562 achieved with k = 29.\n",
      "random forest regressor computed in 0:02:53.301671 seconds.\n",
      "best CV accuracy 0.94082 achieved with max_features = 0.6.\n",
      "ridge regressor computed in 0:00:49.847677 seconds.\n",
      "Best CV accuracy 0.86208 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:31.345278 seconds.\n",
      "best CV accuracy 0.87502 achieved with k = 31.\n",
      "random forest regressor computed in 0:02:55.672485 seconds.\n",
      "best CV accuracy 0.94257 achieved with max_features = 0.6.\n",
      "ridge regressor computed in 0:00:49.555368 seconds.\n",
      "Best CV accuracy 0.79685 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:36.780210 seconds.\n",
      "best CV accuracy 0.81044 achieved with k = 39.\n",
      "random forest regressor computed in 0:02:56.957192 seconds.\n",
      "best CV accuracy 0.89246 achieved with max_features = 0.6.\n",
      "ridge regressor computed in 0:00:49.148458 seconds.\n",
      "Best CV accuracy 0.74295 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:36.260854 seconds.\n",
      "best CV accuracy 0.7624 achieved with k = 33.\n",
      "random forest regressor computed in 0:02:57.138758 seconds.\n",
      "best CV accuracy 0.85551 achieved with max_features = 0.9.\n"
     ]
    }
   ],
   "source": [
    "for subset in subset1_list:\n",
    "    combined_regressor(subset)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regressor computed in 0:01:06.612525 seconds.\n",
      "Best CV accuracy 0.77099 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:39.734586 seconds.\n",
      "best CV accuracy 0.78461 achieved with k = 29.\n",
      "random forest regressor computed in 0:09:35.408833 seconds.\n",
      "best CV accuracy 0.87099 achieved with max_features = 0.4.\n",
      "\n",
      "ridge regressor computed in 0:01:03.232966 seconds.\n",
      "Best CV accuracy 0.79929 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:36.409155 seconds.\n",
      "best CV accuracy 0.81469 achieved with k = 21.\n",
      "random forest regressor computed in 0:09:41.745169 seconds.\n",
      "best CV accuracy 0.89491 achieved with max_features = 0.4.\n",
      "\n",
      "ridge regressor computed in 0:01:05.099701 seconds.\n",
      "Best CV accuracy 0.79356 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:36.695220 seconds.\n",
      "best CV accuracy 0.8123 achieved with k = 25.\n",
      "random forest regressor computed in 0:09:43.101404 seconds.\n",
      "best CV accuracy 0.89408 achieved with max_features = 0.6.\n",
      "\n",
      "ridge regressor computed in 0:01:04.131335 seconds.\n",
      "Best CV accuracy 0.85718 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:35.381680 seconds.\n",
      "best CV accuracy 0.87048 achieved with k = 29.\n",
      "random forest regressor computed in 0:09:40.573092 seconds.\n",
      "best CV accuracy 0.9434 achieved with max_features = 0.6.\n",
      "\n",
      "ridge regressor computed in 0:01:04.438602 seconds.\n",
      "Best CV accuracy 0.82235 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:37.684694 seconds.\n",
      "best CV accuracy 0.83771 achieved with k = 27.\n",
      "random forest regressor computed in 0:09:40.269675 seconds.\n",
      "best CV accuracy 0.91619 achieved with max_features = 0.6.\n",
      "\n",
      "ridge regressor computed in 0:01:04.321416 seconds.\n",
      "Best CV accuracy 0.82834 achieved with alpha = 0.0.\n",
      "knn regressor computed in 0:02:35.590275 seconds.\n",
      "best CV accuracy 0.84022 achieved with k = 23.\n",
      "random forest regressor computed in 0:09:54.751043 seconds.\n",
      "best CV accuracy 0.91797 achieved with max_features = 0.7.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subset in subset2_list:\n",
    "    combined_regressor(subset)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in [subset3_list]:\n",
    "    combined_regressor(subset)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_may_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
